[ { "title": "Kubernetes Pod Warning: 1 node(s) had volume node affinity conflict", "url": "/posts/node-affinity-fix/", "categories": "Kubernetes", "tags": "EKS, Kubernetes", "date": "2022-06-15 00:00:00 +0800", "snippet": "0. If you didn’t find the solution in other answers…In our case the error happened on a AWS EKS cluster freshly provisioned with Pulumi (see full source here). The error drove me nuts, since I didn’t change anything, just created a PersistentVolumeClaim as described in the Buildpacks Tekton docs:apiVersion: v1kind: PersistentVolumeClaimmetadata: name: buildpacks-source-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 500MiI didn’t change anything else from the default EKS configuration and also didn’t add/change any PersistentVolume or StorageClass (in fact I didn’t even know how to do that). As the default EKS setup seems to rely on 2 nodes, I got the error:0/2 nodes are available: 2 node(s) had volume node affinity conflict.Reading through Sownak Roy’s answer I got a first glue what to do - but didn’t know how to do it. So for the folks interested here are all my steps to resolve the error: Warning: This will delete all your data, make sure you have backups.1. Check EKS nodes failure-domain.beta.kubernetes.io labelsAs described in the section Statefull applications in this post two nodes are provisioned on other AWS availability zones as the persistent volume (PV), which is created by applying our PersistendVolumeClaim described above.To check that, you need to look into/describe your nodes with kubectl get nodes:$ kubectl get nodesNAME STATUS ROLES AGE VERSIONip-172-31-10-186.eu-central-1.compute.internal Ready &lt;none&gt; 2d16h v1.21.5-eks-bc4871bip-172-31-20-83.eu-central-1.compute.internal Ready &lt;none&gt; 2d16h v1.21.5-eks-bc4871band then have a look at the Label section using kubectl describe node &lt;node-name&gt;:$ kubectl describe node ip-172-77-88-99.eu-central-1.compute.internalName: ip-172-77-88-99.eu-central-1.compute.internalRoles: &lt;none&gt;Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=t2.medium beta.kubernetes.io/os=linux failure-domain.beta.kubernetes.io/region=eu-central-1 failure-domain.beta.kubernetes.io/zone=eu-central-1b kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-172-77-88-99.eu-central-1.compute.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=t2.medium topology.kubernetes.io/region=eu-central-1 topology.kubernetes.io/zone=eu-central-1bAnnotations: node.alpha.kubernetes.io/ttl: 0...In my case the node ip-172-77-88-99.eu-central-1.compute.internal has failure-domain.beta.kubernetes.io/region defined as eu-central-1 and the az with failure-domain.beta.kubernetes.io/zone to eu-central-1b.And the other node defines failure-domain.beta.kubernetes.io/zone az eu-central-1a:$ kubectl describe nodes ip-172-31-10-186.eu-central-1.compute.internalName: ip-172-31-10-186.eu-central-1.compute.internalRoles: &lt;none&gt;Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/instance-type=t2.medium beta.kubernetes.io/os=linux failure-domain.beta.kubernetes.io/region=eu-central-1 failure-domain.beta.kubernetes.io/zone=eu-central-1a kubernetes.io/arch=amd64 kubernetes.io/hostname=ip-172-31-10-186.eu-central-1.compute.internal kubernetes.io/os=linux node.kubernetes.io/instance-type=t2.medium topology.kubernetes.io/region=eu-central-1 topology.kubernetes.io/zone=eu-central-1aAnnotations: node.alpha.kubernetes.io/ttl: 0...2. Check PersistentVolume’s topology.kubernetes.io fieldNow we should check the PersistentVolume automatically provisioned after we manually applied our PersistentVolumeClaim. Use kubectl get pv:$ kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGEpvc-93650993-6154-4bd0-bd1c-6260e7df49d3 1Gi RWO Delete Bound default/buildpacks-source-pvc gp2 21dfollowed by kubectl describe pv &lt;pv-name&gt;$ kubectl describe pv pvc-93650993-6154-4bd0-bd1c-6260e7df49d3Name: pvc-93650993-6154-4bd0-bd1c-6260e7df49d3Labels: topology.kubernetes.io/region=eu-central-1 topology.kubernetes.io/zone=eu-central-1cAnnotations: kubernetes.io/createdby: aws-ebs-dynamic-provisioner...The PersistentVolume was configured with the label topology.kubernetes.io/zone in az eu-central-1c, which makes our Pods complain about not finding their volume - since they are in a completely different az!3. Add allowedTopologies to StorageClassAs stated in the Kubernetes docs one solution to the problem is to add a allowedTopologies configuration to the StorageClass. If you already provisioned a EKS cluster like me, you need to retrieve your already defined StorageClass withkubectl get storageclasses gp2 -o yamlSave it to a file called storage-class.yml and add a allowedTopologies section that matches your node’s failure-domain.beta.kubernetes.io labels like this:allowedTopologies:- matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - eu-central-1a - eu-central-1bThe allowedTopologies configuration defines that the failure-domain.beta.kubernetes.io/zone of the PersistentVolume must be either in eu-central-1a or eu-central-1b - not eu-central-1c!The full storage-class.yml looks like this:apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: gp2parameters: fsType: ext4 type: gp2provisioner: kubernetes.io/aws-ebsreclaimPolicy: DeletevolumeBindingMode: WaitForFirstConsumerallowedTopologies:- matchLabelExpressions: - key: failure-domain.beta.kubernetes.io/zone values: - eu-central-1a - eu-central-1bApply the enhanced StorageClass configuration to your EKS cluster withkubectl apply -f storage-class.yml4. Delete PersistentVolumeClaim, add storageClassName: gp2 to it and re-apply itIn order to get things working again, we need to delete the PersistentVolumeClaim first.To map the PersistentVolumeClaim to our previously define StorageClass we need to add storageClassName: gp2 to the PersistendVolumeClaim definition in our pvc.yml:apiVersion: v1kind: PersistentVolumeClaimmetadata: name: buildpacks-source-pvcspec: accessModes: - ReadWriteOnce resources: requests: storage: 500Mi storageClassName: gp2Finally re-apply the PersistentVolumeClaim with kubectl apply -f pvc.yml. This should resolve the error." }, { "title": "How to auto connect bluetooth device after Ubuntu Login", "url": "/posts/autoconnect-bluetooth/", "categories": "Blogging, Infrastructure", "tags": "ubuntu, bluetooth autoconnect", "date": "2022-06-15 00:00:00 +0800", "snippet": "After installing Ubuntu 18.04 on my macbook, bluetooth did not worked out of the box. Wasting hours after hours finally I could managed to get bluetooth working.Problem: I paired my bluetooth speaker/mouse. but after restart my laptop, every time I needed to manually connect. It did not auto connect. To solve this issue I followed this commands and it worked.Below Procedure Tested with my JBL XtremeOS: Ubuntu 18.04after login try this.. Open Terminal and run bluetoothctl The Output will be similar to thisOutput:pratap@i7-4770:~$ bluetoothctl[NEW] Controller xx:xx:xx:xx:xx:xx i7-4770 [default][NEW] Device aa:bb:cc:dd:ee:ff JBL Xtreme[NEW] Device xx:xx:xx:xx:xx:xx HUAWEI P smartAgent registered[bluetooth]# In above case “JBL Xtreme” Bluetooth Device is Paired but not yet connected.. So to connect to this devicerun connect aa:bb:cc:dd:ee:ff at the prompt [bluetooth]#Example:[bluetooth]# connect aa:bb:cc:dd:ee:ffAttempting to connect to aa:bb:cc:dd:ee:ff[CHG] Device aa:bb:cc:dd:ee:ff Connected: yesConnection successful[CHG] Device aa:bb:cc:dd:ee:ff ServicesResolved: yes[JBL Xtreme]#This Means if you can run the command bluetoothctl and then at the [bluetooth]# prompt if you can input connect aa:bb:cc:dd:ee:ff The Bluetooth Device will connect.So this can be done with a single command in terminal like this, after your first login open Terminal and run this command.echo \"connect aa:bb:cc:dd:ee:ff\" | bluetoothctlExample:pratap@i7-4770:~$ echo \"connect aa:bb:cc:dd:ee:ff\" | bluetoothctl\\[NEW] Controller xx:xx:xx:xx:xx:xx i7-4770 [default]\\[NEW] Device aa:bb:cc:dd:ee:ff JBL Xtreme\\[NEW] Device xx:xx:xx:xx:xx:xx HUAWEI P smart\\Agent registered\\[bluetooth]# connect aa:bb:cc:dd:ee:ff\\Attempting to connect to aa:bb:cc:dd:ee:ff\\Agent unregistered\\[DEL] Controller xx:xx:xx:xx:xx:xx i7-4770 [default]\\pratap@i7-4770:~$so the command echo \"connect aa:bb:cc:dd:ee:ff\" | bluetoothctl is working..This means if we can run this command at login without human interaction.. the Bluetooth Device which is Paired and already Turned on at the time of Boot will connect in the above manual way.. mkdir ~/bin (Create this directory if you dont have already.. Otherwise Ignore this step) touch ~/bin/btautoconnect.sh gedit ~/bin/btautoconnect.shPaste the Below Content:#!/bin/bashbluetoothctlsleep 10echo “connect aa:bb:cc:dd:ee:ff” | bluetoothctlsleep 12echo “connect aa:bb:cc:dd:ee:ff” | bluetoothctlexit Save &amp; Close the File. chmod +x ~/bin/btautoconnect.shcreate a .desktop file named btautoconnect.desktop in ~/.config/autostart/ touch ~/.config/autostart/btautoconnect.desktopOpen the fiel with gedit and copy paste the content below this command gedit ~/.config/autostart/btautoconnect.desktopContent:[Desktop Entry]Type=ApplicationExec=/bin/bash /home/pratap/bin/btautoconnect.shHidden=falseNoDisplay=falseX-GNOME-Autostart-enabled=trueName=BTAutoConnectX-GNOME-Autostart-Delay=5Comment=Starts Bluetooth speaker Reboot to see the BT Device Connected after login in 10 to 20seconds, without any Human Interaction." }, { "title": "The true cost of Kubernetes: People, Time and Productivity", "url": "/posts/cost-of-k8s/", "categories": "Kubernetes", "tags": "Kubernetes", "date": "2022-05-20 00:00:00 +0800", "snippet": "Kubernetes is a proven technology for container orchestration and is a great tool for running resilient applications, but the true cost is often underestimated. Our comparison explored the opportunity costs related to developer experience and managed responsibilities, this post investigates the actual financial costs of using Kubernetes.When building a production platform, you can select different layers of abstraction. We distinguish four major abstraction layers you can pick from. If you’re not an infrastructure company, we advocate building on top of cloud service provider primitives. Spoiler alert: We estimate that operating Kubernetes clusters costs at least $100k and can easily cost more than $500k annually for a ridiculously low amount of resources.The cost of Kubernetes on Azure, GCP, and AWSKubernetes is an open-source technology, but open-source doesn’t mean it’s free to operate. The operational costs depend on how you choose to use Kubernetes. When opting to use Kubernetes, you have two main options: Self-hosted Kubernetes Managed KubernetesSelf-hosted KubernetesIf you go with a self-hosted Kubernetes solution, you will need to budget for the nodes needed to run your workloads, but you will also need to add the costs of the nodes to run your control plane and of the team to manage the operation.For a reliable Kubernetes production environment, you will need to create and manage nodes that host your control plane, in addition to separate worker nodes to host your application workloads. The control plane is where the main Kubernetes componentsneeded to configure and orchestrate your workloads live. The control plane runs key processes including: kube-api-server to expose the Kubernetes API, etcd, a highly-available key value store, to store all cluster data, kube-schedule, which schedules newly created Pods, and more processes like the kube-controller-manager and cloud-controller-manager.Technically, you could use the same nodes to host the control plane and worker nodes, but it is not recommended for production environments.Self-hosted Kubernetes total costFor this calculation, we decided to do the cost simulation using nodes running on Azure. We did the estimation using D2s v3 Azure VMs machines, which have 2 vCPUs and 8 GB RAM and used 3 nodes which is the minimum for redundancy purposes. With that, you should have 5.4GB of RAM per node, so ~16GB of usable RAM and 6vCPU.Secondly, we took the average DevOps compensation in the US, $141,000 (cited by Builtin) and multiplied it by 4 to represent a team of four engineers, which is needed to properly cover 24/7 operations.As a result, here is what the total cost of ownership (TCO) for self-hosted Kubernetes looks like: Expense Annual Cost Nodes to host the control plane of your clusters $0.096 x 24 x 365 x 3 = $2523 Nodes to host application workload $0.096 x 24 x 365 x 3 = $2523 Compensation for members on your DevOps/SRE team $141,000 x 4 = $564,000 Annual total base cost: $2523 + $2523 + $564,000 = $569,046This cost estimation doesn’t include the sanity of your DevOps team when issues arise, which is, we believe, priceless.Managed Kubernetes on Azure, GCP, or AWSThen there is the managed Kubernetes route. With a managed Kubernetes solution, you do not need to handle creating or managing your Kubernetes clusters, nor do you need to worry about creating the control plane or installing its components like etcd, kube-apiserver, or kube-scheduler.GKE, EKS, and AKS: The cost of the managed control planeThe 3 biggest managed Kubernetes providers are GCP, AWS, and Azure with Google Kubernetes Engine (GKE), Elastic Kubernetes Service (EKS), and Azure Kubernetes Service (AKS). From what we found, the pricing is similar for the managed control plane of all these services at $0.10/hour. Managed Kubernetes Provider Cost to run one cluster GCP GKE $0.10 x 24 x 365 = $876 AWS EKS $0.10 x 24 x 365 = $876 Azure AKS $0.10 x 24 x 365 = $876 For all these offerings, there are no automatic version updates or auto-recovery and you still need to pay for the computing resources like CPU, memory, and ephemeral storage that your worker pods consume.Cost of the worker nodesThe cost of worker nodes also varies across cloud service providers and depends on the amount of computing resources you need as well as in which region of the world your servers are running. We multiply by 3 for redundancy purposes. Provider Resources Annual Cost AWS m6gd.large EC2 instances 2 vCPUs and 8 GB RAM $0.0904 x 24 x 365 x 3 = $2376 Azure D2s v3 VM machines 2 vCPUs and 8 GB RAM $0.096 x 24 x 365 x 3 = $2523 GCP e2-standard-2 VMs 2 vCPUs and 8 GB RAM $0.067 x 24 x 365 x 3 = $1761 With these instances, you should have 5.4GB of RAM per node, so ~16GB of usable RAM and 6vCPU.Managed Kubernetes total costYou also need to account for the maintenance responsibilities not covered. You’ll need at least one dedicated DevOps person to monitor this managed solution and handle the responsibilities not covered by these offerings. Managed Kubernetes Provider Cost to run one cluster Cost of worker nodes Salary for dedicated DevOps TCO AWS EKS $876 $2376 $141,000 $144,252 Azure AKS $876 $2523 $141,000 $144,399 GCP GKE $876 $1761 $141,000 $143,637 This option buys you a lot more time and energy to focus on developing and improving your applications, but you will still be stuck with a lot of the Kubernetes responsibilities and complexity depending on the cloud service provider you use." }, { "title": "Setup Kubernetes Cluster using K3S, MetalLB, LetsEncrypt on Bare Metal", "url": "/posts/setup-k3s/", "categories": "Blogging, Infrastructure", "tags": "k3s, cloud, kubernetes, metallb, nginx-ingress, letsencrypt", "date": "2022-05-01 09:33:00 +0800", "snippet": "Setup K3S ClusterBy default Rancher K3S comes with Traefik 1.7. We will setup K3S without Traefik ingress in this tutorial. Execute below command on master node 1.curl -sfL https://get.k3s.io | sh -s - server --datastore-endpoint=\"mysql://user:pass@tcp(ip_address:3306)/databasename\" --disable traefik --node-taint CriticalAddonsOnly=true:NoExecute --tls-san 192.168.1.2 --tls-san k3s.home.labExecute above command on master node 2 to setup HA.Validate cluster setup:sudo kubectl get nodeNAME STATUS ROLES AGE VERSIONk3s-master-1 Ready master 3m9s v1.18.9+k3s1Make sure you have HA Proxy Setup:########################################################### Kubernetes AP ILB##########################################################frontend kubernetes-frontend bind 192.168.1.2:6443 mode tcp option tcplog default_backend kubernetes-backendbackend kubernetes-backend mode tcp option tcp-check balance roundrobin server k3s-master-1 192.168.1.10:6443 check fall 3 rise 2 server k3s-master-2 192.168.1.20:6443 check fall 3 rise 2 Join worker nodes to K3S ClusterGet node token from one of the master node by executing below command:sudo cat /var/lib/rancher/k3s/server/node-tokenK105c8c5de8deac516ebgd454r45547481d70625ee3e5200acdbe8ea071191debd4::server:gd5de354807077fde4259fd9632ea045454We will use above command output value to join worker nodes:curl -sfL https://get.k3s.io | K3S_URL=https://192.168.1.2:6443 K3S_TOKEN= sh - Validate K3S cluster state:NAME STATUS ROLES AGE VERSIONk3s-master-1 Ready master 15m v1.18.9+k3s1k3s-worker-node-1 Ready &lt;none&gt; 3m44s v1.18.9+k3s1k3s-worker-node-2 Ready &lt;none&gt; 2m52s v1.18.9+k3s1k3s-master-2 Ready master 11m v1.18.9+k3s1MetalLB Setupkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.4/manifests/namespace.yamlkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.4/manifests/metallb.yamlkubectl create secret generic -n metallb-system memberlist --from-literal=secretkey=\"$(openssl rand -base64 128)\"Create a file called metallb-config.yaml and enter below values:apiVersion: v1kind: ConfigMapmetadata: namespace: metallb-system name: configdata: config: | address-pools: - name: default protocol: layer2 addresses: - 192.168.1.240-192.168.1.250Apply changes:sudo kubectl apply -f metallb-config.yamlDeploy sample application with servicekubectl create deploy nginx --image nginxkubectl expose deploy nginx --port 80Check status:kubectl get svc,podsNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.43.0.1 &lt;none&gt; 443/TCP 44mservice/nginx ClusterIP 10.43.14.116 &lt;none&gt; 80/TCP 31sNAME READY STATUS RESTARTS AGEpod/nginx-f89759699-25lpb 1/1 Running 0 59sNginx Ingress setupIn this tutorial, I will be using helm to setup nginx ingress controller. Execute below commands to setup nginx ingress from client machine with helm, kubectl configured:helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginxhelm repo updatehelm install home ingress-nginx/ingress-nginxCheck Ingress controller status:kubectl --namespace default get services -o wide -w home-ingress-nginx-controller Setup Ingress by creating home-ingress.yaml and add below values. Replace example.ioapiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: annotations: kubernetes.io/ingress.class: nginx name: home-ingress namespace: defaultspec: rules: - host: example.io http: paths: - backend: serviceName: nginx servicePort: 80 path: /Execute command to apply: kubectl apply -f home-ingress.yamlCheck Status on Ingress:kubectl get ingNAME CLASS HOSTS ADDRESS PORTS AGEhome-ingress &lt;none&gt; example.io 192.168.1.240 80 8m26sLetsencrypt setup Execute below command to create namespaces, pods, and other related configurations:kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.0.3/cert-manager.yamlOnce above completes lets validate pods status.2. Validate setup:kubectl get pods --namespace cert-managerNAME READY STATUS RESTARTS AGEcert-manager-cainjector-76c9d55b6f-cp2jf 1/1 Running 0 39scert-manager-79c5f9946-qkfzv 1/1 Running 0 38scert-manager-webhook-6d4c5c44bb-4mdgc 1/1 Running 0 38s Setup staging environment by applying below changes. Update email:vi staging_issure.yamland paste below values and save the file:apiVersion: cert-manager.io/v1kind: Issuermetadata: name: letsencrypt-stagingspec: acme: # The ACME server URL server: https://acme-staging-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: john@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # Enable the HTTP-01 challenge provider solvers: - http01: ingress: class: nginxApply changes:kubectl apply -f staging_issure.yaml We will apply production issure later in this tutotial. We should first test SSL settings prior to making changes to use production certificates.SSL setup with LetsEncrypt and Nginx Ingress Before proceeding here, please make sure your dns is setup correctly from your cloud providor or in your homelab to allow traffic from internet. LetsEncrypt uses http validation to issue certificates and it needs to reach correct dns alias from where the cert request has been initiated.Create new ingress file as shown below:vi home-ingress-ssl.yamlCopy and paste in above file:apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: annotations: kubernetes.io/ingress.class: nginx cert-manager.io/issuer: letsencrypt-staging name: home-ingress namespace: defaultspec: tls: - hosts: - example.io secretName: home-example-io-tls rules: - host: example.io http: paths: - backend: serviceName: nginx servicePort: 80 path: /Apply changes:kubectl apply -f home-ingress-ssl.yamlValidate certificate creation:kubectl describe certificateSpec: Dns Names: example.io Issuer Ref: Group: cert-manager.io Kind: Issuer Name: letsencrypt-staging Secret Name: home-example-io-tlsStatus: Conditions: Last Transition Time: 2020-10-26T20:19:15Z Message: Issuing certificate as Secret does not exist Reason: DoesNotExist Status: False Type: Ready Last Transition Time: 2020-10-26T20:19:18Z Message: Issuing certificate as Secret does not exist Reason: DoesNotExist Status: True Type: Issuing Next Private Key Secret Name: home-example-io-tls-76dqgEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Issuing 10s cert-manager Issuing certificate as Secret does not exist Normal Generated 8s cert-manager Stored new private key in temporary Secret resource \"home-example-io-tls-76dqg\" Normal Requested 4s cert-manager Created new CertificateRequest resource \"home-example-io-tls-h98zf\"Now you can browse your dns url and validate certificate. If you see something like below, that means your letsencrypt certificate management has been setup successfully.Set production issure to get valid certificateCreate production issure:vi production-issure.yamlCopy and paste below values to above file. Update email:apiVersion: cert-manager.io/v1kind: Issuermetadata: name: letsencrypt-prodspec: acme: # The ACME server URL server: https://acme-v02.api.letsencrypt.org/directory # Email address used for ACME registration email: user@example.com # Name of a secret used to store the ACME account private key privateKeySecretRef: name: letsencrypt-staging # Enable the HTTP-01 challenge provider solvers: - http01: ingress: class: nginxApply changes:kubectl apply -f production-issure.yamlUpdate home-ingress-ssl.yaml file you created earlier with below values:apiVersion: networking.k8s.io/v1beta1kind: Ingressmetadata: annotations: kubernetes.io/ingress.class: nginx cert-manager.io/issuer: letsencrypt-prod name: home-ingress namespace: defaultspec: tls: - hosts: - example.io secretName: home-example-io-tls rules: - host: example.io http: paths: - backend: serviceName: nginx servicePort: 80 path: /Apply changes:kubectl apply -f home-ingress-ssl.yamlValidate changes: NOTE: Give it sometime as it may take 2-5 mins to get the cert request to complete.kubectl describe certificateYour output should look something like below to get valid certificate.Spec: Dns Names: example.io Issuer Ref: Group: cert-manager.io Kind: Issuer Name: letsencrypt-prod Secret Name: home-example-io-tlsStatus: Conditions: Last Transition Time: 2020-10-26T20:43:35Z Message: Certificate is up to date and has not expired Reason: Ready Status: True Type: Ready Not After: 2021-01-24T19:43:25Z Not Before: 2020-10-26T19:43:25Z Renewal Time: 2020-12-25T19:43:25Z Revision: 2Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Issuing 24m cert-manager Issuing certificate as Secret does not exist Normal Generated 24m cert-manager Stored new private key in temporary Secret resource \"home-example-io-tls-76dqg\" Normal Requested 24m cert-manager Created new CertificateRequest resource \"home-example-io-tls-h98zf\" Normal Issuing 105s cert-manager Issuing certificate as Secret was previously issued by Issuer.cert-manager.io/letsencrypt-staging Normal Reused 103s cert-manager Reusing private key stored in existing Secret resource \"home-example-io-tls\" Normal Requested 100s cert-manager Created new CertificateRequest resource \"home-example-io-tls-ccxgf\" Normal Issuing 30s (x2 over 23m) cert-manager The certificate has been successfully issuedBrowse your application and check for valid certificate. If it looks something like below, that means you have successfully requested valid certificate from letsencrypt certificate authority." } ]
